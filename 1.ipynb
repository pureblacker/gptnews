{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1469]), torch.Size([5850]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=[]\n",
    "import torch\n",
    "# for i in a:\n",
    "#     n.append(len(i['title']))\n",
    "\n",
    "for i in a:\n",
    "    n.append(len(i['content']))\n",
    "\n",
    "# title中都很干净，content中有多余空格和换行符。是否去空格和换行符\n",
    "# 标题平均长度13-14,最大长度40,最小6。取标题最大长度多少？>25的只有19个，大于20的只有205个\n",
    "# 内容平均长度1292,最大9628,最小187,小于1000的只有1/4的样子,但是GPTchinese最大长度是1024，怎样减少数据丢失呢？？\n",
    "    # 一种方案。。。\n",
    "# '[Space]'是干啥的\n",
    "\n",
    "# 惊了惊了，encoded怎么返回的维度是(1,maxlen)，坑壁\n",
    "\n",
    "# min(n)\n",
    "n=torch.tensor(n,dtype=torch.long)\n",
    "n[n<=1024].shape,n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1024])\n",
      "tensor([[[0, 0, 0,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "from data_set import GPT2Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained('./vocab/vocab.txt')\n",
    "# self,tokenizer,max_len,title_max_len,data_dir,data_set_name,path_file=None,is_overwrite=False):\n",
    "dataset=GPT2Dataset(tokenizer=tokenizer,max_len=1024,path_file='./data_dir/dev.json',title_max_len=0,data_dir='',is_overwrite=False,data_set_name='')\n",
    "dataloader=DataLoader(dataset,batch_size=2)\n",
    "for idx,batch in enumerate(dataloader):\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['token_type_ids'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=batch['input_ids'].reshape(2,1024,)\n",
    "token_type=batch['token_type_ids'].reshape(2,1024,)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,GPT2Config\n",
    "from model import MyGPT2LMHeadModel\n",
    "config=GPT2Config.from_json_file('./model/config.json')\n",
    "model=MyGPT2LMHeadModel(config)\n",
    "tokenizer=BertTokenizer.from_pretrained('vocab/vocab.txt')\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.forward(input_ids=inputs,token_type_ids=token_type,labels=inputs,title_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "filter_logits= out[1][...,-1,:]\n",
    "next_tokens = torch.multinomial(F.softmax(filter_logits, dim=-1), num_samples=1)\n",
    "if next_tokens==[121]*next_tokens.size(0):\n",
    "    print(44)\n",
    "else:\n",
    "    print(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1009]), torch.Size([827]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputlist=[0 for i in range(2)]\n",
    "for i in range(2):\n",
    "    for j in range(21128):\n",
    "        if token_type[i][j]==1:\n",
    "            break\n",
    "    inputlist[i]=inputs[i][:j]\n",
    "from torch  import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "def top_k_top_p_filtering(logits, filter_value=-float(\"Inf\")):\n",
    "    \n",
    "    top_k=5\n",
    "    top_p=0.95\n",
    "\n",
    "    assert logits.dim() == 1   # logits的维度为1，size:[vocab_size]\n",
    "\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    #将概率小于第k大概率的元素都变为-inf，即概率变成极小，永远不会随机抽到\n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]    #< 后面表示的是topk里面最小的值\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    # 如果top_p不为0，则将在logits中保留概率值累积达到top_p的标记\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "        # 对排序后的结果使用softmax归一化，再获取累积概率序列\n",
    "        # 例如：原始序列排序且softmax后[0.4, 0.3, 0.2, 0.1]，则变为：[0.4, 0.7, 0.9, 1.0]，如果top_p=0.9,则cumu..=[f,f,f,t]，最后会删掉最后一个\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)      #[0.3,0.35,0.4,..,0.95,0.951,0.952,...1]\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p                            #删除累积概率高于top_p的标记，[false,....,true,true,true,true,true]\n",
    "        # 将索引向右移动，使第一个标记也保持在top_p之上()  （就是保证不删掉第一个）\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() #不加省略号也可以，加是为了当生成多个结果时\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        #取得要被删去的ids\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]    \n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def evaluate(model, device, test_data):\n",
    "    \"\"\"\n",
    "    对测试数据集进行模型测试\n",
    "    Args:\n",
    "        model: 模型\n",
    "        device: 设备信息\n",
    "        test_data: 测试数据类\n",
    "        args: 训练参数配置信息\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    tokenizer=BertTokenizer.from_pretrained('vocab/vocab.txt')\n",
    "    length=40\n",
    "    title_id = 1            #....................\n",
    "\n",
    "    repetition_penalty=1.2\n",
    "    unk_id = tokenizer.convert_tokens_to_ids(\"[UNK]\")\n",
    "    sep_id = tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "    model.eval()\n",
    "\n",
    "    rougescore1=[]\n",
    "    rougescore2=[]\n",
    "    rougescorel=[]\n",
    "    with torch.no_grad():\n",
    "        for idx in range(1):\n",
    "            token_type_id = test_data[idx][\"token_type_ids\"].reshape(1024,)\n",
    "            input_id=test_data[idx][\"input_ids\"].reshape(1024,)\n",
    "            for j in range(1024):\n",
    "                if token_type_id[j]==1:\n",
    "                    break\n",
    "            for k in range(j,1024):\n",
    "                if token_type_id[k]==0:\n",
    "                    break\n",
    "            label_id=input_id[j:k].contiguous()\n",
    "            input_id=input_id[:j].contiguous()\n",
    "            print(label_id)\n",
    "            print(''.join(tokenizer.convert_ids_to_tokens(label_id)))\n",
    "            generated_ids=[]\n",
    "            for _ in range(length):\n",
    "                # 获取预测结果\n",
    "                outputs = model.forward(input_ids=input_id)\n",
    "                # 获取预测文本\n",
    "                next_token_logits = outputs[0][-1,:]\n",
    "                already_token_ids =set([ids for ids in generated_ids])\n",
    "                for token_id in already_token_ids:\n",
    "                    next_token_logits[token_id] /= repetition_penalty\n",
    "                \n",
    "                next_token_logits[unk_id] = -float(\"Inf\")\n",
    "                filter_logits = top_k_top_p_filtering(next_token_logits)\n",
    "                next_tokens = torch.multinomial(F.softmax(filter_logits, dim=-1), num_samples=1)   #multinomial对张量的每一行进行num_samples次取样\n",
    "                if next_tokens==sep_id:\n",
    "                    break\n",
    "                generated_ids.append(next_tokens.item())\n",
    "                input_id = torch.cat((input_id, next_tokens), dim=-1)\n",
    "                if len(input_id)>=1024:\n",
    "                    input_id=input_id[-1023:]  \n",
    "            predict=\"\".join(tokenizer.convert_ids_to_tokens(generated_ids)).replace(\"##\", \"\").replace(\"[SEP]\", \" \").replace(\"[UNK]\", \"\") \n",
    "            label=\"\".join(tokenizer.convert_ids_to_tokens(label_id)).replace(\"##\", \"\").replace(\"[UNK]\", \"\").replace(\"[SEP]\", \" \")  \n",
    "            print(predict, label)\n",
    "            rouge = Rouge()\n",
    "            rouge_score = rouge.get_scores(predict, label)\n",
    "            rougescore1.append(rouge_score[0][\"rouge-1\"]['f'])\n",
    "            gescore2.append(rouge_score[0][\"rouge-2\"]['f'])\n",
    "            rougescorel.append(rouge_score[0][\"rouge-l\"]['f'])\n",
    "    return (sum(rouge_score1)/len(rouge_score1),sum(rouge_score2)/len(rouge_score2),sum(rouge_scorel)/len(rouge_scorel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 704, 1744, 2898, 5330, 1355, 1213, 1181, 1121, 1059, 4413,  100, 6619,\n",
      "        2099,  100])\n",
      "中国持续发力削减全球[UNK]赤字[UNK]\n",
      "局蕴兮buffet▇said3ce赃pk10francis贖疤世jump迫湍祚訶ːぬ瘙創婦昕进篑臨湖辑憬ŋ搓願询hd鐮羅廠激1903 中国持续发力削减全球赤字\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rou' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\YaliZhu\\Desktop\\ruanjianbei\\gptnews\\1.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000013?line=0'>1</a>\u001b[0m evaluate(model,\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m,dataset)\n",
      "\u001b[1;32mc:\\Users\\YaliZhu\\Desktop\\ruanjianbei\\gptnews\\1.ipynb Cell 10'\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, device, test_data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000012?line=60'>61</a>\u001b[0m rouge_score \u001b[39m=\u001b[39m rouge\u001b[39m.\u001b[39mget_scores(predict, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000012?line=61'>62</a>\u001b[0m rougescore1\u001b[39m.\u001b[39mappend(rouge_score[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrouge-1\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000012?line=62'>63</a>\u001b[0m rou\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000012?line=63'>64</a>\u001b[0m gescore2\u001b[39m.\u001b[39mappend(rouge_score[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrouge-2\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000012?line=64'>65</a>\u001b[0m rougescorel\u001b[39m.\u001b[39mappend(rouge_score[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrouge-l\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rou' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(model,'cpu',dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 0.5714285714285714, 'p': 0.6666666666666666, 'f': 0.6153846104142012}\n",
      "{'r': 0.3333333333333333, 'p': 0.4, 'f': 0.36363635867768596}\n",
      "{'r': 0.5714285714285714, 'p': 0.6666666666666666, 'f': 0.6153846104142012}\n"
     ]
    }
   ],
   "source": [
    "# coding:utf8\n",
    "from rouge import Rouge\n",
    "a = \"i am a student from china\"  # 预测摘要 （可以是列表也可以是句子）\n",
    "b = \"i am student from school on japan\" #真实摘要\n",
    " \n",
    "'''\n",
    "f:F1值  p：查准率  R：召回率\n",
    "'''\n",
    "rouge = Rouge()\n",
    "rouge_score = rouge.get_scores(a, b)\n",
    "print(rouge_score[0][\"rouge-1\"])\n",
    "print(rouge_score[0][\"rouge-2\"])\n",
    "print(rouge_score[0][\"rouge-l\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7fb8039a8f54b6eaa53411e1600db4762c5d3778c5768c8cb602a31c798a2c38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fb8039a8f54b6eaa53411e1600db4762c5d3778c5768c8cb602a31c798a2c38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
