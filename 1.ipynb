{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1643: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['近',\n",
       " '期',\n",
       " '，',\n",
       " '美',\n",
       " '国',\n",
       " '国',\n",
       " '会',\n",
       " '众',\n",
       " '院',\n",
       " '通',\n",
       " '过',\n",
       " '法',\n",
       " '案',\n",
       " '，',\n",
       " '重',\n",
       " '申',\n",
       " '美',\n",
       " '国',\n",
       " '对',\n",
       " '台',\n",
       " '湾',\n",
       " '的',\n",
       " '承',\n",
       " '诺',\n",
       " '。',\n",
       " '对',\n",
       " '此',\n",
       " '，',\n",
       " '中',\n",
       " '国',\n",
       " '外',\n",
       " '交',\n",
       " '部',\n",
       " '发',\n",
       " '言',\n",
       " '人',\n",
       " '表',\n",
       " '示',\n",
       " '，',\n",
       " '有',\n",
       " '关',\n",
       " '法',\n",
       " '案',\n",
       " '严',\n",
       " '重',\n",
       " '违',\n",
       " '反',\n",
       " '一',\n",
       " '个',\n",
       " '中',\n",
       " '国',\n",
       " '原',\n",
       " '则',\n",
       " '和',\n",
       " '中',\n",
       " '美',\n",
       " '三',\n",
       " '个',\n",
       " '联',\n",
       " '合',\n",
       " '公',\n",
       " '报',\n",
       " '规',\n",
       " '定',\n",
       " '，',\n",
       " '粗',\n",
       " '暴',\n",
       " '干',\n",
       " '涉',\n",
       " '中',\n",
       " '国',\n",
       " '内',\n",
       " '政',\n",
       " '，',\n",
       " '中',\n",
       " '方',\n",
       " '对',\n",
       " '此',\n",
       " '坚',\n",
       " '决',\n",
       " '反',\n",
       " '对',\n",
       " '并',\n",
       " '已',\n",
       " '向',\n",
       " '美',\n",
       " '方',\n",
       " '提',\n",
       " '出',\n",
       " '严',\n",
       " '正',\n",
       " '交',\n",
       " '涉',\n",
       " '。',\n",
       " '事',\n",
       " '实',\n",
       " '上',\n",
       " '，',\n",
       " '中',\n",
       " '国',\n",
       " '高',\n",
       " '度',\n",
       " '关',\n",
       " '注',\n",
       " '美',\n",
       " '国',\n",
       " '国',\n",
       " '内',\n",
       " '打',\n",
       " '[UNK]',\n",
       " '台',\n",
       " '湾',\n",
       " '牌',\n",
       " '[UNK]',\n",
       " '、',\n",
       " '挑',\n",
       " '战',\n",
       " '一',\n",
       " '中',\n",
       " '原',\n",
       " '则',\n",
       " '的',\n",
       " '危',\n",
       " '险',\n",
       " '动',\n",
       " '向',\n",
       " '。',\n",
       " '近',\n",
       " '年',\n",
       " '来',\n",
       " '，',\n",
       " '作',\n",
       " '为',\n",
       " '[UNK]',\n",
       " '亲',\n",
       " '台',\n",
       " '[UNK]',\n",
       " '势',\n",
       " '力',\n",
       " '大',\n",
       " '本',\n",
       " '营',\n",
       " '的',\n",
       " '美',\n",
       " '国',\n",
       " '国',\n",
       " '会',\n",
       " '动',\n",
       " '作',\n",
       " '不',\n",
       " '断',\n",
       " '，',\n",
       " '先',\n",
       " '后',\n",
       " '通',\n",
       " '过',\n",
       " '[UNK]',\n",
       " '与',\n",
       " '台',\n",
       " '湾',\n",
       " '交',\n",
       " '往',\n",
       " '法',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '亚',\n",
       " '洲',\n",
       " '再',\n",
       " '保',\n",
       " '证',\n",
       " '倡',\n",
       " '议',\n",
       " '法',\n",
       " '[UNK]',\n",
       " '等',\n",
       " '一',\n",
       " '系',\n",
       " '列',\n",
       " '[UNK]',\n",
       " '挺',\n",
       " '台',\n",
       " '[UNK]',\n",
       " '法',\n",
       " '案',\n",
       " '，',\n",
       " '[UNK]',\n",
       " '2019',\n",
       " '财',\n",
       " '年',\n",
       " '国',\n",
       " '防',\n",
       " '授',\n",
       " '权',\n",
       " '法',\n",
       " '案',\n",
       " '[UNK]',\n",
       " '也',\n",
       " '多',\n",
       " '处',\n",
       " '触',\n",
       " '及',\n",
       " '台',\n",
       " '湾',\n",
       " '问',\n",
       " '题',\n",
       " '。',\n",
       " '今',\n",
       " '年',\n",
       " '3',\n",
       " '月',\n",
       " '，',\n",
       " '美',\n",
       " '参',\n",
       " '院',\n",
       " '亲',\n",
       " '台',\n",
       " '议',\n",
       " '员',\n",
       " '再',\n",
       " '抛',\n",
       " '[UNK]',\n",
       " '台',\n",
       " '湾',\n",
       " '保',\n",
       " '证',\n",
       " '法',\n",
       " '[UNK]',\n",
       " '草',\n",
       " '案',\n",
       " '。',\n",
       " '众',\n",
       " '院',\n",
       " '议',\n",
       " '员',\n",
       " '继',\n",
       " '而',\n",
       " '在',\n",
       " '4',\n",
       " '月',\n",
       " '提',\n",
       " '出',\n",
       " '众',\n",
       " '院',\n",
       " '版',\n",
       " '的',\n",
       " '草',\n",
       " '案',\n",
       " '并',\n",
       " '在',\n",
       " '近',\n",
       " '期',\n",
       " '通',\n",
       " '过',\n",
       " '。',\n",
       " '上',\n",
       " '述',\n",
       " '法',\n",
       " '案',\n",
       " '的',\n",
       " '核',\n",
       " '心',\n",
       " '目',\n",
       " '标',\n",
       " '是',\n",
       " '强',\n",
       " '化',\n",
       " '美',\n",
       " '台',\n",
       " '关',\n",
       " '系',\n",
       " '，',\n",
       " '并',\n",
       " '将',\n",
       " '台',\n",
       " '作',\n",
       " '为',\n",
       " '美',\n",
       " '[UNK]',\n",
       " '印',\n",
       " '太',\n",
       " '战',\n",
       " '略',\n",
       " '[UNK]',\n",
       " '的',\n",
       " '重',\n",
       " '要',\n",
       " '伙',\n",
       " '伴',\n",
       " '。',\n",
       " '同',\n",
       " '时',\n",
       " '，',\n",
       " '[UNK]',\n",
       " '亲',\n",
       " '台',\n",
       " '[UNK]',\n",
       " '议',\n",
       " '员',\n",
       " '还',\n",
       " '有',\n",
       " '意',\n",
       " '制',\n",
       " '造',\n",
       " '事',\n",
       " '端',\n",
       " '。',\n",
       " '今',\n",
       " '年',\n",
       " '2',\n",
       " '月',\n",
       " '，',\n",
       " '5',\n",
       " '名',\n",
       " '共',\n",
       " '和',\n",
       " '党',\n",
       " '参',\n",
       " '议',\n",
       " '员',\n",
       " '致',\n",
       " '信',\n",
       " '众',\n",
       " '议',\n",
       " '院',\n",
       " '议',\n",
       " '长',\n",
       " '，',\n",
       " '促',\n",
       " '其',\n",
       " '邀',\n",
       " '请',\n",
       " '台',\n",
       " '湾',\n",
       " '地',\n",
       " '区',\n",
       " '领',\n",
       " '导',\n",
       " '人',\n",
       " '在',\n",
       " '国',\n",
       " '会',\n",
       " '上',\n",
       " '发',\n",
       " '表',\n",
       " '讲',\n",
       " '话',\n",
       " '。',\n",
       " '这',\n",
       " '一',\n",
       " '动',\n",
       " '议',\n",
       " '显',\n",
       " '然',\n",
       " '有',\n",
       " '悖',\n",
       " '于',\n",
       " '美',\n",
       " '国',\n",
       " '与',\n",
       " '台',\n",
       " '湾',\n",
       " '的',\n",
       " '非',\n",
       " '官',\n",
       " '方',\n",
       " '关',\n",
       " '系',\n",
       " '，',\n",
       " '其',\n",
       " '用',\n",
       " '心',\n",
       " '是',\n",
       " '实',\n",
       " '质',\n",
       " '性',\n",
       " '改',\n",
       " '变',\n",
       " '美',\n",
       " '台',\n",
       " '关',\n",
       " '系',\n",
       " '定',\n",
       " '位',\n",
       " '。',\n",
       " '上',\n",
       " '述',\n",
       " '动',\n",
       " '向',\n",
       " '出',\n",
       " '现',\n",
       " '并',\n",
       " '非',\n",
       " '偶',\n",
       " '然',\n",
       " '。',\n",
       " '在',\n",
       " '中',\n",
       " '美',\n",
       " '建',\n",
       " '交',\n",
       " '40',\n",
       " '周',\n",
       " '年',\n",
       " '之',\n",
       " '际',\n",
       " '，',\n",
       " '两',\n",
       " '国',\n",
       " '关',\n",
       " '系',\n",
       " '摩',\n",
       " '擦',\n",
       " '加',\n",
       " '剧',\n",
       " '，',\n",
       " '所',\n",
       " '谓',\n",
       " '[UNK]',\n",
       " '中',\n",
       " '国',\n",
       " '威',\n",
       " '胁',\n",
       " '论',\n",
       " '[UNK]',\n",
       " '再',\n",
       " '次',\n",
       " '沉',\n",
       " '渣',\n",
       " '泛',\n",
       " '起',\n",
       " '。',\n",
       " '美',\n",
       " '国',\n",
       " '对',\n",
       " '华',\n",
       " '认',\n",
       " '知',\n",
       " '出',\n",
       " '现',\n",
       " '严',\n",
       " '重',\n",
       " '偏',\n",
       " '差',\n",
       " '，',\n",
       " '对',\n",
       " '华',\n",
       " '政',\n",
       " '策',\n",
       " '中',\n",
       " '负',\n",
       " '面',\n",
       " '因',\n",
       " '素',\n",
       " '上',\n",
       " '升',\n",
       " '，',\n",
       " '保',\n",
       " '守',\n",
       " '人',\n",
       " '士',\n",
       " '甚',\n",
       " '至',\n",
       " '成',\n",
       " '立',\n",
       " '了',\n",
       " '[UNK]',\n",
       " '当',\n",
       " '前',\n",
       " '中',\n",
       " '国',\n",
       " '威',\n",
       " '胁',\n",
       " '委',\n",
       " '员',\n",
       " '会',\n",
       " '[UNK]',\n",
       " '。',\n",
       " '在',\n",
       " '此',\n",
       " '背',\n",
       " '景',\n",
       " '下',\n",
       " '，',\n",
       " '美',\n",
       " '国',\n",
       " '将',\n",
       " '台',\n",
       " '海',\n",
       " '关',\n",
       " '系',\n",
       " '作',\n",
       " '为',\n",
       " '战',\n",
       " '略',\n",
       " '抓',\n",
       " '手',\n",
       " '，',\n",
       " '通',\n",
       " '过',\n",
       " '打',\n",
       " '[UNK]',\n",
       " '台',\n",
       " '湾',\n",
       " '牌',\n",
       " '[UNK]',\n",
       " '在',\n",
       " '双',\n",
       " '边',\n",
       " '关',\n",
       " '系',\n",
       " '中',\n",
       " '增',\n",
       " '加',\n",
       " '筹',\n",
       " '码',\n",
       " '。',\n",
       " '特',\n",
       " '朗',\n",
       " '普',\n",
       " '就',\n",
       " '任',\n",
       " '后',\n",
       " '，',\n",
       " '国',\n",
       " '会',\n",
       " '对',\n",
       " '总',\n",
       " '统',\n",
       " '外',\n",
       " '交',\n",
       " '政',\n",
       " '策',\n",
       " '的',\n",
       " '约',\n",
       " '束',\n",
       " '力',\n",
       " '和',\n",
       " '塑',\n",
       " '造',\n",
       " '力',\n",
       " '加',\n",
       " '强',\n",
       " '。',\n",
       " '其',\n",
       " '实',\n",
       " '国',\n",
       " '会',\n",
       " '推',\n",
       " '动',\n",
       " '通',\n",
       " '过',\n",
       " '涉',\n",
       " '台',\n",
       " '法',\n",
       " '案',\n",
       " '对',\n",
       " '行',\n",
       " '政',\n",
       " '部',\n",
       " '门',\n",
       " '不',\n",
       " '具',\n",
       " '约',\n",
       " '束',\n",
       " '力',\n",
       " '，',\n",
       " '美',\n",
       " '政',\n",
       " '府',\n",
       " '在',\n",
       " '2018',\n",
       " '年',\n",
       " '并',\n",
       " '未',\n",
       " '提',\n",
       " '升',\n",
       " '美',\n",
       " '台',\n",
       " '官',\n",
       " '员',\n",
       " '互',\n",
       " '访',\n",
       " '级',\n",
       " '别',\n",
       " '，',\n",
       " '美',\n",
       " '军',\n",
       " '舰',\n",
       " '也',\n",
       " '没',\n",
       " '有',\n",
       " '[UNK]',\n",
       " '访',\n",
       " '问',\n",
       " '[UNK]',\n",
       " '台',\n",
       " '湾',\n",
       " '港',\n",
       " '口',\n",
       " '，',\n",
       " '保',\n",
       " '持',\n",
       " '着',\n",
       " '某',\n",
       " '种',\n",
       " '克',\n",
       " '制',\n",
       " '。',\n",
       " '但',\n",
       " '从',\n",
       " '美',\n",
       " '总',\n",
       " '统',\n",
       " '签',\n",
       " '署',\n",
       " '国',\n",
       " '会',\n",
       " '通',\n",
       " '过',\n",
       " '的',\n",
       " '法',\n",
       " '案',\n",
       " '可',\n",
       " '以',\n",
       " '看',\n",
       " '出',\n",
       " '，',\n",
       " '国',\n",
       " '会',\n",
       " '对',\n",
       " '外',\n",
       " '交',\n",
       " '产',\n",
       " '生',\n",
       " '了',\n",
       " '影',\n",
       " '响',\n",
       " '。',\n",
       " '立',\n",
       " '法',\n",
       " '也',\n",
       " '为',\n",
       " '政',\n",
       " '府',\n",
       " '对',\n",
       " '台',\n",
       " '政',\n",
       " '策',\n",
       " '提',\n",
       " '供',\n",
       " '更',\n",
       " '大',\n",
       " '空',\n",
       " '间',\n",
       " '。',\n",
       " '然',\n",
       " '而',\n",
       " '，',\n",
       " '美',\n",
       " '国',\n",
       " '需',\n",
       " '要',\n",
       " '认',\n",
       " '真',\n",
       " '衡',\n",
       " '量',\n",
       " '打',\n",
       " '[UNK]',\n",
       " '台',\n",
       " '湾',\n",
       " '牌',\n",
       " '[UNK]',\n",
       " '成',\n",
       " '本',\n",
       " '。',\n",
       " '首',\n",
       " '先',\n",
       " '是',\n",
       " '美',\n",
       " '国',\n",
       " '应',\n",
       " '对',\n",
       " '危',\n",
       " '机',\n",
       " '的',\n",
       " '代',\n",
       " '价',\n",
       " '。',\n",
       " '美',\n",
       " '方',\n",
       " '官',\n",
       " '员',\n",
       " '和',\n",
       " '学',\n",
       " '者',\n",
       " '已',\n",
       " '明',\n",
       " '确',\n",
       " '发',\n",
       " '出',\n",
       " '警',\n",
       " '告',\n",
       " '，',\n",
       " '美',\n",
       " '国',\n",
       " '卷',\n",
       " '入',\n",
       " '台',\n",
       " '湾',\n",
       " '问',\n",
       " '题',\n",
       " '得',\n",
       " '不',\n",
       " '偿',\n",
       " '失',\n",
       " '。',\n",
       " '美',\n",
       " '国',\n",
       " '学',\n",
       " '者',\n",
       " '曾',\n",
       " '在',\n",
       " '媒',\n",
       " '体',\n",
       " '发',\n",
       " '文',\n",
       " '指',\n",
       " '出',\n",
       " '，',\n",
       " '如',\n",
       " '果',\n",
       " '台',\n",
       " '海',\n",
       " '爆',\n",
       " '发',\n",
       " '危',\n",
       " '机',\n",
       " '，',\n",
       " '美',\n",
       " '国',\n",
       " '可',\n",
       " '能',\n",
       " '需',\n",
       " '要',\n",
       " '[UNK]',\n",
       " '援',\n",
       " '助',\n",
       " '[UNK]',\n",
       " '台',\n",
       " '湾',\n",
       " '，',\n",
       " '进',\n",
       " '而',\n",
       " '导',\n",
       " '致',\n",
       " '新',\n",
       " '的',\n",
       " '冷',\n",
       " '战',\n",
       " '乃',\n",
       " '至',\n",
       " '与',\n",
       " '中',\n",
       " '国',\n",
       " '大',\n",
       " '陆',\n",
       " '的',\n",
       " '冲',\n",
       " '突',\n",
       " '。',\n",
       " '但',\n",
       " '如',\n",
       " '果',\n",
       " '美',\n",
       " '国',\n",
       " '让',\n",
       " '台',\n",
       " '湾',\n",
       " '自',\n",
       " '己',\n",
       " '面',\n",
       " '对',\n",
       " '，',\n",
       " '则',\n",
       " '有',\n",
       " '损',\n",
       " '美',\n",
       " '国',\n",
       " '的',\n",
       " '信',\n",
       " '誉',\n",
       " '，',\n",
       " '影',\n",
       " '响',\n",
       " '美',\n",
       " '盟',\n",
       " '友',\n",
       " '对',\n",
       " '同',\n",
       " '盟',\n",
       " '关',\n",
       " '系',\n",
       " '的',\n",
       " '支',\n",
       " '持',\n",
       " '。',\n",
       " '其',\n",
       " '次',\n",
       " '是',\n",
       " '对',\n",
       " '中',\n",
       " '美',\n",
       " '关',\n",
       " '系',\n",
       " '的',\n",
       " '危',\n",
       " '害',\n",
       " '。',\n",
       " '历',\n",
       " '史',\n",
       " '证',\n",
       " '明',\n",
       " '，',\n",
       " '中',\n",
       " '美',\n",
       " '合',\n",
       " '则',\n",
       " '两',\n",
       " '利',\n",
       " '、',\n",
       " '斗',\n",
       " '则',\n",
       " '两',\n",
       " '伤',\n",
       " '。',\n",
       " '中',\n",
       " '美',\n",
       " '关',\n",
       " '系',\n",
       " '是',\n",
       " '当',\n",
       " '今',\n",
       " '世',\n",
       " '界',\n",
       " '最',\n",
       " '重',\n",
       " '要',\n",
       " '的',\n",
       " '双',\n",
       " '边',\n",
       " '关',\n",
       " '系',\n",
       " '之',\n",
       " '一',\n",
       " '，',\n",
       " '保',\n",
       " '持',\n",
       " '中',\n",
       " '美',\n",
       " '关',\n",
       " '系',\n",
       " '的',\n",
       " '稳',\n",
       " '定',\n",
       " '发',\n",
       " '展',\n",
       " '，',\n",
       " '不',\n",
       " '仅',\n",
       " '符',\n",
       " '合',\n",
       " '两',\n",
       " '国',\n",
       " '和',\n",
       " '两',\n",
       " '国',\n",
       " '人',\n",
       " '民',\n",
       " '的',\n",
       " '根',\n",
       " '本',\n",
       " '利',\n",
       " '益',\n",
       " '，',\n",
       " '也',\n",
       " '是',\n",
       " '国',\n",
       " '际',\n",
       " '社',\n",
       " '会',\n",
       " '的',\n",
       " '普',\n",
       " '遍',\n",
       " '期',\n",
       " '待',\n",
       " '。',\n",
       " '美',\n",
       " '国',\n",
       " '蓄',\n",
       " '意',\n",
       " '挑',\n",
       " '战',\n",
       " '台',\n",
       " '湾',\n",
       " '问',\n",
       " '题',\n",
       " '的',\n",
       " '底',\n",
       " '线',\n",
       " '，',\n",
       " '加',\n",
       " '剧',\n",
       " '中',\n",
       " '美',\n",
       " '关',\n",
       " '系',\n",
       " '的',\n",
       " '复',\n",
       " '杂',\n",
       " '性',\n",
       " '和',\n",
       " '不',\n",
       " '确',\n",
       " '定',\n",
       " '性',\n",
       " '，',\n",
       " '损',\n",
       " '害',\n",
       " '两',\n",
       " '国',\n",
       " '在',\n",
       " '重',\n",
       " '要',\n",
       " '领',\n",
       " '域',\n",
       " '合',\n",
       " '作',\n",
       " '，',\n",
       " '损',\n",
       " '人',\n",
       " '又',\n",
       " '害',\n",
       " '己',\n",
       " '。',\n",
       " '美',\n",
       " '国',\n",
       " '打',\n",
       " '[UNK]',\n",
       " '台',\n",
       " '湾',\n",
       " '牌',\n",
       " '[UNK]',\n",
       " '是',\n",
       " '一',\n",
       " '场',\n",
       " '危',\n",
       " '险',\n",
       " '的',\n",
       " '赌',\n",
       " '博',\n",
       " '。',\n",
       " '台',\n",
       " '湾',\n",
       " '问',\n",
       " '题',\n",
       " '是',\n",
       " '中',\n",
       " '国',\n",
       " '核',\n",
       " '心',\n",
       " '利',\n",
       " '益',\n",
       " '，',\n",
       " '中',\n",
       " '国',\n",
       " '政',\n",
       " '府',\n",
       " '和',\n",
       " '人',\n",
       " '民',\n",
       " '决',\n",
       " '不',\n",
       " '会',\n",
       " '对',\n",
       " '此',\n",
       " '坐',\n",
       " '视',\n",
       " '不',\n",
       " '理',\n",
       " '。',\n",
       " '中',\n",
       " '国',\n",
       " '敦',\n",
       " '促',\n",
       " '美',\n",
       " '方',\n",
       " '恪',\n",
       " '守',\n",
       " '一',\n",
       " '个',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "with open('./data_dir/train.json','r',encoding='utf-8') as f:\n",
    "    a=json.load(f)\n",
    "tokenizer=BertTokenizer.from_pretrained('./vocab/vocab.txt')\n",
    "conten_tokens=tokenizer.tokenize(a[0]['content'].replace(' ','[Space]'))\n",
    "conten_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1469]), torch.Size([5850]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=[]\n",
    "import torch\n",
    "# for i in a:\n",
    "#     n.append(len(i['title']))\n",
    "\n",
    "for i in a:\n",
    "    n.append(len(i['content']))\n",
    "\n",
    "# title中都很干净，content中有多余空格和换行符。是否去空格和换行符\n",
    "# 标题平均长度13-14,最大长度40,最小6。取标题最大长度多少？>25的只有19个，大于20的只有205个\n",
    "# 内容平均长度1292,最大9628,最小187,小于1000的只有1/4的样子,但是GPTchinese最大长度是1024，怎样减少数据丢失呢？？\n",
    "    # 一种方案。。。\n",
    "# '[Space]'是干啥的\n",
    "\n",
    "# min(n)\n",
    "n=torch.tensor(n,dtype=torch.long)\n",
    "n[n<=1024].shape,n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded=tokenizer.encode_plus(a[0]['content'],a[0]['title'],max_length=1024,truncation='only_first',padding='max_length',return_token_type_ids=True,return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 6818, 3309,  ..., 6603, 1300,  102]]),\n",
       " tensor([[0, 0, 0,  ..., 1, 1, 1]]),\n",
       " 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded['input_ids'],encoded['token_type_ids'],encoded['attention_mask'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_id=1\n",
    "mask=(encoded['token_type_ids']==title_id).long() \n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 1024])\n",
      "tensor([[[0, 0, 0,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 1, 1]]])\n"
     ]
    }
   ],
   "source": [
    "from data_set import GPT2Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "# self,tokenizer,max_len,title_max_len,data_dir,data_set_name,path_file=None,is_overwrite=False):\n",
    "      \n",
    "dataset=GPT2Dataset(tokenizer=tokenizer,max_len=1024,path_file='./data_dir/dev.json',title_max_len=0,data_dir='',is_overwrite=False,data_set_name='')\n",
    "dataloader=DataLoader(dataset,batch_size=4)\n",
    "for idx,batch in enumerate(dataloader):\n",
    "    print(batch['input_ids'].shape)\n",
    "    print(batch['token_type_ids'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1024])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=batch['input_ids'].contiguous()\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\YaliZhu\\Desktop\\ruanjianbei\\gptnews\\1.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000009?line=0'>1</a>\u001b[0m input_ids\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:79] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\YaliZhu\\Desktop\\ruanjianbei\\gptnews\\1.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000010?line=1'>2</a>\u001b[0m model_config \u001b[39m=\u001b[39m GPT2Config\u001b[39m.\u001b[39mfrom_json_file(\u001b[39m'\u001b[39m\u001b[39m./model/config.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000010?line=2'>3</a>\u001b[0m tr\u001b[39m=\u001b[39mGPT2Model(config\u001b[39m=\u001b[39mmodel_config)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/YaliZhu/Desktop/ruanjianbei/gptnews/1.ipynb#ch0000010?line=3'>4</a>\u001b[0m out \u001b[39m=\u001b[39m tr(batch[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], past_key_values\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, token_type_ids\u001b[39m=\u001b[39;49mbatch[\u001b[39m'\u001b[39;49m\u001b[39mtoken_type_ids\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:890\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=879'>880</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=880'>881</a>\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=881'>882</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=886'>887</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=887'>888</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=888'>889</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=889'>890</a>\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=890'>891</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=891'>892</a>\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=892'>893</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=893'>894</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=894'>895</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=895'>896</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=896'>897</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=897'>898</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=898'>899</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=900'>901</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=901'>902</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:395\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=392'>393</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=393'>394</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=394'>395</a>\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=395'>396</a>\u001b[0m     hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=396'>397</a>\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=397'>398</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=398'>399</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=399'>400</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=400'>401</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=401'>402</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=402'>403</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=403'>404</a>\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1046'>1047</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1047'>1048</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1049'>1050</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1050'>1051</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1051'>1052</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/torch/nn/modules/module.py?line=1052'>1053</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:336\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=333'>334</a>\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=334'>335</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=335'>336</a>\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attn(query, key, value, attention_mask, head_mask)\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=337'>338</a>\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=338'>339</a>\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n",
      "File \u001b[1;32md:\\Anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:193\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=191'>192</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_attn\u001b[39m(\u001b[39mself\u001b[39m, query, key, value, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, head_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=192'>193</a>\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmatmul(query, key\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m))\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=194'>195</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_attn_weights:\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/torch/lib/site-packages/transformers/models/gpt2/modeling_gpt2.py?line=195'>196</a>\u001b[0m         attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m/\u001b[39m (\u001b[39mfloat\u001b[39m(value\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:79] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model,GPT2Config\n",
    "model_config = GPT2Config.from_json_file('./model/config.json')\n",
    "tr=GPT2Model(config=model_config)\n",
    "out = tr(batch['input_ids'], past_key_values=None, token_type_ids=batch['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fb8039a8f54b6eaa53411e1600db4762c5d3778c5768c8cb602a31c798a2c38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
